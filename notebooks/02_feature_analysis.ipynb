{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1aac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# NOTEBOOK 2: ANALYSE ET ING√âNIERIE DES FEATURES\n",
    "# ====================================================================\n",
    "# Ce notebook se concentre sur l'analyse approfondie des features,\n",
    "# la cr√©ation de nouvelles variables et la s√©lection des meilleures\n",
    "# caract√©ristiques pour la mod√©lisation ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bda16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# IMPORTS ET CONFIGURATION\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f0ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ ANALYSE ET ING√âNIERIE DES FEATURES\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 108) (data_loader.py, line 104)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:3699\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[1]\u001b[39m\u001b[92m, line 40\u001b[39m\n    from data.data_loader import DataLoader\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32mc:\\ML Drilling Oil & Gaz\\ML-Drilling-Oil-Gaz\\notebooks\\../src\\data\\__init__.py:47\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom .data_loader import DataLoader\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32mc:\\ML Drilling Oil & Gaz\\ML-Drilling-Oil-Gaz\\notebooks\\../src\\data\\data_loader.py:104\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated triple-quoted string literal (detected at line 108)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML imports\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_regression, f_classif, RFE, \n",
    "    SelectFromModel, mutual_info_regression, mutual_info_classif\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Statistiques\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, chi2_contingency\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"üî¨ ANALYSE ET ING√âNIERIE DES FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ajouter le path pour nos modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.data_loader import DataLoader\n",
    "from data.data_preprocessor import DataPreprocessor\n",
    "from data.feature_engineering import DrillingFeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CHARGEMENT ET PR√âPARATION DES DONN√âES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d086577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chargement des donn√©es...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä Chargement des donn√©es...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialiser les composants\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m loader = \u001b[43mDataLoader\u001b[49m({\u001b[33m'\u001b[39m\u001b[33mdata_path\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m      5\u001b[39m preprocessor = DataPreprocessor()\n\u001b[32m      6\u001b[39m feature_engineer = DrillingFeatureEngineerFeatureEngineer()\n",
      "\u001b[31mNameError\u001b[39m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"üìä Chargement des donn√©es...\")\n",
    "\n",
    "# Initialiser les composants\n",
    "loader = DataLoader({'data_path': '../data'})\n",
    "preprocessor = DataPreprocessor()\n",
    "feature_engineer = DrillingFeatureEngineerFeatureEngineer()\n",
    "\n",
    "# Charger les donn√©es\n",
    "try:\n",
    "    formation_df = loader.load_formation_data()\n",
    "    kick_df = loader.load_kick_detection_data()\n",
    "    print(\"‚úÖ Donn√©es r√©elles charg√©es\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Utilisation de donn√©es synth√©tiques\")\n",
    "    synthetic_data = loader.load_synthetic_drilling_data(n_samples=5000, random_seed=42)\n",
    "    formation_df = synthetic_data.copy()\n",
    "    kick_df = synthetic_data.copy()\n",
    "\n",
    "print(f\"Formation data shape: {formation_df.shape}\")\n",
    "print(f\"Kick detection data shape: {kick_df.shape}\")\n",
    "\n",
    "# Nettoyage de base\n",
    "formation_df = preprocessor.handle_missing_values(formation_df)\n",
    "kick_df = preprocessor.handle_missing_values(kick_df)\n",
    "\n",
    "print(\"‚úÖ Donn√©es nettoy√©es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b430fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 1. ANALYSE DE L'IMPORTANCE DES FEATURES EXISTANTES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "681b0166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ ANALYSE DE L'IMPORTANCE DES FEATURES EXISTANTES\n",
      "--------------------------------------------------\n",
      "üéØ Importance des features - Formation Pressure:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 200\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Analyser l'importance pour les diff√©rentes variables cibles\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müéØ Importance des features - Formation Pressure:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mFormationPressure\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformation_df\u001b[49m.columns:\n\u001b[32m    201\u001b[39m     formation_importance = analyze_feature_importance_regression(formation_df, \u001b[33m'\u001b[39m\u001b[33mFormationPressure\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    203\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ Importance des features - ROP:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'formation_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüéØ ANALYSE DE L'IMPORTANCE DES FEATURES EXISTANTES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def analyze_feature_importance_regression(df, target_col, max_features=10):\n",
    "    \"\"\"Analyse l'importance des features pour un probl√®me de r√©gression\"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Variable cible '{target_col}' non trouv√©e\")\n",
    "        return None\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        print(\"‚ö†Ô∏è Pas de features num√©riques disponibles\")\n",
    "        return None\n",
    "    \n",
    "    X = df[feature_cols].dropna()\n",
    "    y = df.loc[X.index, target_col]\n",
    "    \n",
    "    print(f\"Analyse pour {target_col} avec {len(feature_cols)} features\")\n",
    "    \n",
    "    # 1. Corr√©lation de Pearson\n",
    "    correlations = []\n",
    "    for feature in feature_cols:\n",
    "        corr, p_value = pearsonr(X[feature], y)\n",
    "        correlations.append({\n",
    "            'feature': feature,\n",
    "            'correlation': abs(corr),\n",
    "            'p_value': p_value,\n",
    "            'method': 'Pearson'\n",
    "        })\n",
    "    \n",
    "    # 2. Information mutuelle\n",
    "    try:\n",
    "        mi_scores = mutual_info_regression(X, y, random_state=42)\n",
    "        for i, feature in enumerate(feature_cols):\n",
    "            correlations.append({\n",
    "                'feature': feature,\n",
    "                'correlation': mi_scores[i],\n",
    "                'p_value': np.nan,\n",
    "                'method': 'Mutual_Info'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur information mutuelle: {e}\")\n",
    "    \n",
    "    # 3. F-score\n",
    "    try:\n",
    "        f_scores, f_p_values = f_regression(X, y)\n",
    "        for i, feature in enumerate(feature_cols):\n",
    "            correlations.append({\n",
    "                'feature': feature,\n",
    "                'correlation': f_scores[i],\n",
    "                'p_value': f_p_values[i],\n",
    "                'method': 'F_Score'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur F-score: {e}\")\n",
    "    \n",
    "    # 4. Random Forest Feature Importance\n",
    "    try:\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        for i, feature in enumerate(feature_cols):\n",
    "            correlations.append({\n",
    "                'feature': feature,\n",
    "                'correlation': rf.feature_importances_[i],\n",
    "                'p_value': np.nan,\n",
    "                'method': 'RF_Importance'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur Random Forest: {e}\")\n",
    "    \n",
    "    # Cr√©er DataFrame des r√©sultats\n",
    "    results_df = pd.DataFrame(correlations)\n",
    "    \n",
    "    # Visualiser les r√©sultats\n",
    "    methods = results_df['method'].unique()\n",
    "    n_methods = len(methods)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_methods, figsize=(6*n_methods, 8))\n",
    "    if n_methods == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        method_data = results_df[results_df['method'] == method].sort\n",
    "        method_data = results_df[results_df['method'] == method].sort_values('correlation', ascending=False)\n",
    "        top_features = method_data.head(max_features)\n",
    "        \n",
    "        axes[i].barh(range(len(top_features)), top_features['correlation'])\n",
    "        axes[i].set_yticks(range(len(top_features)))\n",
    "        axes[i].set_yticklabels(top_features['feature'])\n",
    "        axes[i].set_title(f'{method}\\nImportance des Features')\n",
    "        axes[i].set_xlabel('Score d\\'Importance')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Inverser l'ordre pour avoir le plus important en haut\n",
    "        axes[i].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def analyze_feature_importance_classification(df, target_col, max_features=10):\n",
    "    \"\"\"Analyse l'importance des features pour un probl√®me de classification\"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Variable cible '{target_col}' non trouv√©e\")\n",
    "        return None\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "    \n",
    "    if not feature_cols:\n",
    "        print(\"‚ö†Ô∏è Pas de features num√©riques disponibles\")\n",
    "        return None\n",
    "    \n",
    "    X = df[feature_cols].dropna()\n",
    "    y = df.loc[X.index, target_col]\n",
    "    \n",
    "    # S'assurer que y est entier pour classification\n",
    "    if y.dtype not in ['int64', 'int32']:\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    print(f\"Analyse pour {target_col} avec {len(feature_cols)} features\")\n",
    "    print(f\"Classes: {np.unique(y)}\")\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    # 1. Information mutuelle\n",
    "    try:\n",
    "        mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "        for i, feature in enumerate(feature_cols):\n",
    "            correlations.append({\n",
    "                'feature': feature,\n",
    "                'score': mi_scores[i],\n",
    "                'method': 'Mutual_Info'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur information mutuelle: {e}\")\n",
    "    \n",
    "    # 2. Chi2 score\n",
    "    try:\n",
    "        # Normaliser les donn√©es pour chi2 (doivent √™tre positives)\n",
    "        X_positive = X - X.min() + 1\n",
    "        chi2_scores, chi2_p_values = f_classif(X_positive, y)\n",
    "        for i, feature in enumerate(feature_cols):\n",
    "            correlations.append({\n",
    "                'feature': feature,\n",
    "                'score': chi2_scores[i],\n",
    "                'method': 'Chi2_Score'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur Chi2: {e}\")\n",
    "    \n",
    "    # 3. Random Forest Feature Importance\n",
    "    try:\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        for i, feature in enumerate(feature_cols):\n",
    "            correlations.append({\n",
    "                'feature': feature,\n",
    "                'score': rf.feature_importances_[i],\n",
    "                'method': 'RF_Importance'\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur Random Forest: {e}\")\n",
    "    \n",
    "    # Visualiser les r√©sultats\n",
    "    if correlations:\n",
    "        results_df = pd.DataFrame(correlations)\n",
    "        methods = results_df['method'].unique()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, len(methods), figsize=(6*len(methods), 8))\n",
    "        if len(methods) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, method in enumerate(methods):\n",
    "            method_data = results_df[results_df['method'] == method].sort_values('score', ascending=False)\n",
    "            top_features = method_data.head(max_features)\n",
    "            \n",
    "            axes[i].barh(range(len(top_features)), top_features['score'])\n",
    "            axes[i].set_yticks(range(len(top_features)))\n",
    "            axes[i].set_yticklabels(top_features['feature'])\n",
    "            axes[i].set_title(f'{method}\\nImportance des Features')\n",
    "            axes[i].set_xlabel('Score d\\'Importance')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Analyser l'importance pour les diff√©rentes variables cibles\n",
    "print(\"üéØ Importance des features - Formation Pressure:\")\n",
    "if 'FormationPressure' in formation_df.columns:\n",
    "    formation_importance = analyze_feature_importance_regression(formation_df, 'FormationPressure')\n",
    "\n",
    "print(\"\\nüéØ Importance des features - ROP:\")\n",
    "if 'ROP' in formation_df.columns:\n",
    "    rop_importance = analyze_feature_importance_regression(formation_df, 'ROP')\n",
    "\n",
    "print(\"\\nüéØ Importance des features - Kick Detection:\")\n",
    "if 'Kick' in kick_df.columns:\n",
    "    kick_importance = analyze_feature_importance_classification(kick_df, 'Kick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ed8fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 2. CR√âATION DE NOUVELLES FEATURES (FEATURE ENGINEERING)\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62049e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß ING√âNIERIE DES FEATURES\n",
      "--------------------------------------------------\n",
      "üîß Ing√©nierie des features pour Formation Data:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Cr√©er les features engineered\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîß Ing√©nierie des features pour Formation Data:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m formation_engineered = create_drilling_features(\u001b[43mformation_df\u001b[49m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîß Ing√©nierie des features pour Kick Detection Data:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     82\u001b[39m kick_engineered = create_drilling_features(kick_df)\n",
      "\u001b[31mNameError\u001b[39m: name 'formation_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüîß ING√âNIERIE DES FEATURES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def create_drilling_features(df):\n",
    "    \"\"\"Cr√©e des features sp√©cifiques au domaine du forage\"\"\"\n",
    "    df_engineered = df.copy()\n",
    "    \n",
    "    print(\"üî® Cr√©ation de features d√©riv√©es...\")\n",
    "    \n",
    "    # Features temporelles si timestamp disponible\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df_engineered = feature_engineer.create_temporal_features(df_engineered, 'Timestamp')\n",
    "        print(\"  ‚úÖ Features temporelles cr√©√©es\")\n",
    "    \n",
    "    # Features de moyennes mobiles\n",
    "    rolling_cols = [col for col in ['WOB', 'RPM', 'ROP', 'FlowRate'] if col in df.columns]\n",
    "    if rolling_cols:\n",
    "        df_engineered = feature_engineer.create_rolling_features(df_engineered, rolling_cols, [3, 5, 10])\n",
    "        print(\"  ‚úÖ Moyennes mobiles cr√©√©es\")\n",
    "    \n",
    "    # Features de lag\n",
    "    lag_cols = [col for col in ['WOB', 'RPM'] if col in df.columns]\n",
    "    if lag_cols:\n",
    "        df_engineered = feature_engineer.create_lag_features(df_engineered, lag_cols, [1, 2])\n",
    "        print(\"  ‚úÖ Features de lag cr√©√©es\")\n",
    "    \n",
    "    # Features de ratios\n",
    "    ratio_pairs = []\n",
    "    if 'WOB' in df.columns and 'RPM' in df.columns:\n",
    "        ratio_pairs.append(('WOB', 'RPM'))\n",
    "    if 'FlowRateIn' in df.columns and 'FlowRateOut' in df.columns:\n",
    "        ratio_pairs.append(('FlowRateIn', 'FlowRateOut'))\n",
    "    if 'ROP' in df.columns and 'WOB' in df.columns:\n",
    "        ratio_pairs.append(('ROP', 'WOB'))\n",
    "    \n",
    "    if ratio_pairs:\n",
    "        df_engineered = feature_engineer.create_ratio_features(df_engineered, ratio_pairs)\n",
    "        print(\"  ‚úÖ Features de ratios cr√©√©es\")\n",
    "    \n",
    "    # Features d'efficacit√© de forage\n",
    "    df_engineered = feature_engineer.create_drilling_efficiency_features(df_engineered)\n",
    "    print(\"  ‚úÖ Features d'efficacit√© cr√©√©es\")\n",
    "    \n",
    "    # Features polynomiales pour variables importantes\n",
    "    important_cols = [col for col in ['WOB', 'RPM'] if col in df.columns]\n",
    "    if len(important_cols) >= 2:\n",
    "        for col in important_cols[:2]:  # Limiter pour √©viter l'explosion dimensionnelle\n",
    "            df_engineered[f'{col}_squared'] = df_engineered[col] ** 2\n",
    "            df_engineered[f'{col}_log'] = np.log1p(np.abs(df_engineered[col]))\n",
    "        print(\"  ‚úÖ Features polynomiales cr√©√©es\")\n",
    "    \n",
    "    # Features d'interaction entre variables importantes\n",
    "    if 'WOB' in df.columns and 'RPM' in df.columns:\n",
    "        df_engineered['WOB_RPM_interaction'] = df_engineered['WOB'] * df_engineered['RPM']\n",
    "    if 'FlowRate' in df.columns and 'MudWeight' in df.columns:\n",
    "        df_engineered['FlowRate_MudWeight_interaction'] = df_engineered['FlowRate'] * df_engineered['MudWeight']\n",
    "    print(\"  ‚úÖ Features d'interaction cr√©√©es\")\n",
    "    \n",
    "    # Features statistiques sur fen√™tre glissante\n",
    "    stat_cols = [col for col in ['WOB', 'RPM', 'ROP'] if col in df.columns]\n",
    "    for col in stat_cols:\n",
    "        if len(df_engineered) > 10:\n",
    "            df_engineered[f'{col}_rolling_std'] = df_engineered[col].rolling(5, min_periods=1).std()\n",
    "            df_engineered[f'{col}_rolling_min'] = df_engineered[col].rolling(5, min_periods=1).min()\n",
    "            df_engineered[f'{col}_rolling_max'] = df_engineered[col].rolling(5, min_periods=1).max()\n",
    "    print(\"  ‚úÖ Features statistiques cr√©√©es\")\n",
    "    \n",
    "    # Nettoyage des valeurs infinies et NaN\n",
    "    df_engineered = df_engineered.replace([np.inf, -np.inf], np.nan)\n",
    "    df_engineered = df_engineered.fillna(df_engineered.median())\n",
    "    \n",
    "    print(f\"üìä Features cr√©√©es: {len(df_engineered.columns) - len(df.columns)} nouvelles features\")\n",
    "    print(f\"üìä Total features: {len(df_engineered.columns)}\")\n",
    "    \n",
    "    return df_engineered\n",
    "\n",
    "# Cr√©er les features engineered\n",
    "print(\"üîß Ing√©nierie des features pour Formation Data:\")\n",
    "formation_engineered = create_drilling_features(formation_df)\n",
    "\n",
    "print(\"\\nüîß Ing√©nierie des features pour Kick Detection Data:\")\n",
    "kick_engineered = create_drilling_features(kick_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 3. S√âLECTION DES MEILLEURES FEATURES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f90a1961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ S√âLECTION DES MEILLEURES FEATURES\n",
      "--------------------------------------------------\n",
      "üéØ S√©lection pour Formation Pressure:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formation_engineered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# S√©lectionner les meilleures features\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müéØ S√©lection pour Formation Pressure:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mFormationPressure\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformation_engineered\u001b[49m.columns:\n\u001b[32m    118\u001b[39m     formation_best_features, formation_methods = select_best_features(\n\u001b[32m    119\u001b[39m         formation_engineered, \u001b[33m'\u001b[39m\u001b[33mFormationPressure\u001b[39m\u001b[33m'\u001b[39m, n_features=\u001b[32m15\u001b[39m\n\u001b[32m    120\u001b[39m     )\n\u001b[32m    122\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ S√©lection pour ROP:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'formation_engineered' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüéØ S√âLECTION DES MEILLEURES FEATURES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def select_best_features(df, target_col, n_features=15, method='combined'):\n",
    "    \"\"\"S√©lectionne les meilleures features selon diff√©rentes m√©thodes\"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Variable cible '{target_col}' non trouv√©e\")\n",
    "        return None, None\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    feature_cols = [col for col in df.columns if col != target_col and \n",
    "                   col not in ['Timestamp'] and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    X = df[feature_cols].fillna(df[feature_cols].median())\n",
    "    y = df[target_col].fillna(df[target_col].median())\n",
    "    \n",
    "    print(f\"S√©lection de {n_features} features parmi {len(feature_cols)} disponibles\")\n",
    "    \n",
    "    selected_features = {}\n",
    "    \n",
    "    # M√©thode 1: Corr√©lation avec la cible\n",
    "    correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    selected_features['correlation'] = correlations.head(n_features).index.tolist()\n",
    "    \n",
    "    # M√©thode 2: Univariate Feature Selection\n",
    "    if pd.api.types.is_numeric_dtype(y) and y.nunique() > 10:  # R√©gression\n",
    "        selector = SelectKBest(score_func=f_regression, k=n_features)\n",
    "        selector.fit(X, y)\n",
    "        selected_features['univariate'] = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
    "    else:  # Classification\n",
    "        y_encoded = LabelEncoder().fit_transform(y.astype(str))\n",
    "        selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "        selector.fit(X, y_encoded)\n",
    "        selected_features['univariate'] = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
    "    \n",
    "    # M√©thode 3: Recursive Feature Elimination avec Random Forest\n",
    "    try:\n",
    "        if pd.api.types.is_numeric_dtype(y) and y.nunique() > 10:\n",
    "            estimator = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        else:\n",
    "            estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            y = LabelEncoder().fit_transform(y.astype(str))\n",
    "        \n",
    "        rfe = RFE(estimator=estimator, n_features_to_select=n_features)\n",
    "        rfe.fit(X, y)\n",
    "        selected_features['rfe'] = [feature_cols[i] for i in rfe.get_support(indices=True)]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur RFE: {e}\")\n",
    "    \n",
    "    # M√©thode 4: Lasso pour r√©gularisation\n",
    "    try:\n",
    "        if pd.api.types.is_numeric_dtype(y) and y.nunique() > 10:\n",
    "            lasso = Lasso(alpha=0.01, random_state=42)\n",
    "            lasso.fit(X, y)\n",
    "            lasso_features = X.columns[lasso.coef_ != 0]\n",
    "        else:\n",
    "            y_encoded = LabelEncoder().fit_transform(y.astype(str))\n",
    "            lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=42, C=1.0)\n",
    "            lasso.fit(X, y_encoded)\n",
    "            lasso_features = X.columns[lasso.coef_[0] != 0]\n",
    "        \n",
    "        selected_features['lasso'] = lasso_features.tolist()[:n_features]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur Lasso: {e}\")\n",
    "    \n",
    "    # M√©thode combin√©e: consensus\n",
    "    if method == 'combined':\n",
    "        feature_counts = {}\n",
    "        for method_name, features in selected_features.items():\n",
    "            for feature in features:\n",
    "                feature_counts[feature] = feature_counts.get(feature, 0) + 1\n",
    "        \n",
    "        # Trier par nombre de votes\n",
    "        consensus_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        final_features = [feature for feature, count in consensus_features[:n_features]]\n",
    "        \n",
    "        print(f\"üìä M√©thode combin√©e utilis√©e\")\n",
    "        print(f\"Features les plus consensuelles:\")\n",
    "        for i, (feature, count) in enumerate(consensus_features[:10]):\n",
    "            print(f\"  {i+1:2d}. {feature}: {count}/{len(selected_features)} votes\")\n",
    "    \n",
    "    else:\n",
    "        final_features = selected_features.get(method, selected_features['correlation'])\n",
    "    \n",
    "    # Visualiser la s√©lection\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    if method == 'combined' and len(consensus_features) > 0:\n",
    "        features_to_plot = consensus_features[:20]  # Top 20\n",
    "        features, votes = zip(*features_to_plot)\n",
    "        \n",
    "        bars = ax.barh(range(len(features)), votes)\n",
    "        ax.set_yticks(range(len(features)))\n",
    "        ax.set_yticklabels(features)\n",
    "        ax.set_xlabel('Nombre de votes')\n",
    "        ax.set_title(f'üéØ S√©lection des Features - M√©thode Combin√©e\\n(Top {len(features)} features)')\n",
    "        \n",
    "        # Colorer diff√©remment selon le nombre de votes\n",
    "        max_votes = max(votes)\n",
    "        for i, (bar, vote) in enumerate(zip(bars, votes)):\n",
    "            if vote == max_votes:\n",
    "                bar.set_color('#2ca02c')  # Vert pour consensus fort\n",
    "            elif vote >= max_votes * 0.7:\n",
    "                bar.set_color('#ff7f0e')  # Orange pour consensus moyen\n",
    "            else:\n",
    "                bar.set_color('#1f77b4')  # Bleu pour consensus faible\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return final_features, selected_features\n",
    "\n",
    "# S√©lectionner les meilleures features\n",
    "print(\"üéØ S√©lection pour Formation Pressure:\")\n",
    "if 'FormationPressure' in formation_engineered.columns:\n",
    "    formation_best_features, formation_methods = select_best_features(\n",
    "        formation_engineered, 'FormationPressure', n_features=15\n",
    "    )\n",
    "\n",
    "print(\"\\nüéØ S√©lection pour ROP:\")\n",
    "if 'ROP' in formation_engineered.columns:\n",
    "    rop_best_features, rop_methods = select_best_features(\n",
    "        formation_engineered, 'ROP', n_features=15\n",
    "    )\n",
    "\n",
    "print(\"\\nüéØ S√©lection pour Kick Detection:\")\n",
    "if 'Kick' in kick_engineered.columns:\n",
    "    kick_best_features, kick_methods = select_best_features(\n",
    "        kick_engineered, 'Kick', n_features=15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 4. ANALYSE DIMENSIONNELLE (PCA)\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6232a237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìê ANALYSE DIMENSIONNELLE (PCA)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formation_engineered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pca, X_pca, scaler\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Effectuer l'analyse PCA\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mFormationPressure\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformation_engineered\u001b[49m.columns:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìê PCA pour Formation Pressure:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m     formation_pca, formation_pca_data, formation_scaler = perform_pca_analysis(\n\u001b[32m     86\u001b[39m         formation_engineered, \u001b[33m'\u001b[39m\u001b[33mFormationPressure\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     87\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'formation_engineered' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìê ANALYSE DIMENSIONNELLE (PCA)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def perform_pca_analysis(df, target_col, n_components=10):\n",
    "    \"\"\"Effectue une analyse PCA\"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    feature_cols = [col for col in df.columns if col != target_col and \n",
    "                   col not in ['Timestamp'] and pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    X = df[feature_cols].fillna(df[feature_cols].median())\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Normaliser les donn√©es\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=min(n_components, len(feature_cols)))\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"PCA effectu√©e: {len(feature_cols)} ‚Üí {pca.n_components_} composantes\")\n",
    "    print(f\"Variance expliqu√©e totale: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "    \n",
    "    # Visualisations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Variance expliqu√©e\n",
    "    axes[0, 0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "                   pca.explained_variance_ratio_, 'bo-')\n",
    "    axes[0, 0].plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "                   pca.explained_variance_ratio_.cumsum(), 'ro-')\n",
    "    axes[0, 0].set_xlabel('Composante')\n",
    "    axes[0, 0].set_ylabel('Variance Expliqu√©e')\n",
    "    axes[0, 0].set_title('Variance Expliqu√©e par Composante')\n",
    "    axes[0, 0].legend(['Individuelle', 'Cumulative'])\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Projection 2D des donn√©es\n",
    "    if pca.n_components_ >= 2:\n",
    "        scatter = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=y, alpha=0.6, cmap='viridis')\n",
    "        axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        axes[0, 1].set_title('Projection PCA 2D')\n",
    "        plt.colorbar(scatter, ax=axes[0, 1])\n",
    "    \n",
    "    # 3. Contribution des features originales\n",
    "    if pca.n_components_ >= 1:\n",
    "        pc1_contributions = pca.components_[0]\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'pc1_contribution': abs(pc1_contributions)\n",
    "        }).sort_values('pc1_contribution', ascending=True).tail(10)\n",
    "        \n",
    "        axes[1, 0].barh(range(len(feature_importance)), feature_importance['pc1_contribution'])\n",
    "        axes[1, 0].set_yticks(range(len(feature_importance)))\n",
    "        axes[1, 0].set_yticklabels(feature_importance['feature'])\n",
    "        axes[1, 0].set_xlabel('Contribution Absolue')\n",
    "        axes[1, 0].set_title('Top 10 Features - PC1')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Biplot pour PC1 vs PC2\n",
    "    if pca.n_components_ >= 2:\n",
    "        for i, feature in enumerate(feature_cols[:10]):  # Top 10 seulement\n",
    "            axes[1, 1].arrow(0, 0, pca.components_[0, i], pca.components_[1, i], \n",
    "                           head_width=0.01, head_length=0.01, alpha=0.7)\n",
    "            axes[1, 1].text(pca.components_[0, i]*1.1, pca.components_[1, i]*1.1, \n",
    "                           feature, fontsize=8)\n",
    "        \n",
    "        axes[1, 1].set_xlabel('PC1')\n",
    "        axes[1, 1].set_ylabel('PC2')\n",
    "        axes[1, 1].set_title('Biplot PCA (Top 10 features)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, X_pca, scaler\n",
    "\n",
    "# Effectuer l'analyse PCA\n",
    "if 'FormationPressure' in formation_engineered.columns:\n",
    "    print(\"üìê PCA pour Formation Pressure:\")\n",
    "    formation_pca, formation_pca_data, formation_scaler = perform_pca_analysis(\n",
    "        formation_engineered, 'FormationPressure'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11296d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 5. ANALYSE DE COLIN√âARIT√â\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cca1806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó ANALYSE DE COLIN√âARIT√â\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüîó ANALYSE DE COLIN√âARIT√â\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def analyze_multicollinearity(df, features=None, threshold=0.8):\n",
    "    \"\"\"Analyse la multicolin√©arit√© entre features\"\"\"\n",
    "    if features is None:\n",
    "        features = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Matrice de corr√©lation\n",
    "    corr_matrix = df[features].corr()\n",
    "    \n",
    "    # Trouver les paires hautement corr√©l√©es\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = abs(corr_matrix.iloc[i, j])\n",
    "            if corr_val > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'feature1': corr_matrix.columns[i],\n",
    "                    'feature2': corr_matrix.columns[j],\n",
    "                    'correlation': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    print(f\"üîç Paires de features avec corr√©lation > {threshold}:\")\n",
    "    if high_corr_pairs:\n",
    "        for pair in sorted(high_corr_pairs, key=lambda x: abs(x['correlation']), reverse=True):\n",
    "            print(f\"  ‚Ä¢ {pair['feature1']} ‚Üî {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ Aucune corr√©lation √©lev√©e d√©tect√©e\")\n",
    "    \n",
    "    # Calcul du VIF (Variance Inflation Factor) si statsmodels disponible\n",
    "    try:\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        \n",
    "        X = df[features].fillna(df[features].median())\n",
    "        \n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = X.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "        vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüìä Variance Inflation Factor (VIF):\")\n",
    "        print(\"   VIF > 10: Multicolin√©arit√© √©lev√©e\")\n",
    "        print(\"   VIF > 5:  Multicolin√©arit√© mod√©r√©e\")\n",
    "        \n",
    "        for _, row in vif_data.head(10).iterrows():\n",
    "            status = \"üî¥\" if row['VIF'] > 10 else \"üü°\" if row['VIF'] > 5 else \"üü¢\"\n",
    "            print(f\"   {status} {row['Feature']}: {row['VIF']:.2f}\")\n",
    "        \n",
    "        # Visualisation\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "        \n",
    "        # Heatmap de corr√©lation\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdYlBu_r', center=0,\n",
    "                   ax=axes[0], square=True, linewidths=0.5)\n",
    "        axes[0].set_title('Matrice de Corr√©lation')\n",
    "        \n",
    "        # VIF plot\n",
    "        top_vif = vif_data.head(15)\n",
    "        colors = ['red' if vif > 10 else 'orange' if vif > 5 else 'green' \n",
    "                 for vif in top_vif['VIF']]\n",
    "        axes[1].barh(range(len(top_vif)), top_vif['VIF'], color=colors, alpha=0.7)\n",
    "        axes[1].set_yticks(range(len(top_vif)))\n",
    "        axes[1].set_yticklabels(top_vif['Feature'])\n",
    "        axes[1].set_xlabel('VIF')\n",
    "        axes[1].set_title('Variance Inflation Factor')\n",
    "        axes[1].axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='VIF = 5')\n",
    "        axes[1].axvline(x=10, color='red', linestyle='--', alpha=0.7, label='VIF = 10')\n",
    "        axes[1].legend()\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return high_corr_pairs, vif_data\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è statsmodels non disponible pour le calcul du VIF\")\n",
    "        return high_corr_pairs, None\n",
    "\n",
    "# Analyser la multicolin√©arit√© pour les features s√©lectionn√©es\n",
    "if 'formation_best_features' in locals() and formation_best_features:\n",
    "    print(\"üîó Multicolin√©arit√© - Formation Features:\")\n",
    "    formation_multicoll, formation_vif = analyze_multicollinearity(\n",
    "        formation_engineered, formation_best_features\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 6. RECOMMANDATIONS FINALES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e88e44db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù RECOMMANDATIONS FINALES\n",
      "==================================================\n",
      "\n",
      "üíæ SAUVEGARDE DES DONN√âES PREPROCESS√âES\n",
      "--------------------------------------------------\n",
      "üíæ Sauvegard√©: ../data/processed/selected_features.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìù RECOMMANDATIONS FINALES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def generate_feature_recommendations(df, best_features, target_col):\n",
    "    \"\"\"G√©n√®re des recommandations pour les features\"\"\"\n",
    "    print(f\"\\nüéØ RECOMMANDATIONS POUR {target_col}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if not best_features:\n",
    "        print(\"‚ö†Ô∏è Aucune feature s√©lectionn√©e\")\n",
    "        return\n",
    "    \n",
    "    print(f\"‚úÖ Features recommand√©es ({len(best_features)}):\")\n",
    "    for i, feature in enumerate(best_features[:10], 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "    \n",
    "    # Analyser les types de features\n",
    "    original_features = [f for f in best_features if not any(\n",
    "        suffix in f for suffix in ['_rolling_', '_lag_', '_squared', '_log', '_ratio', '_interaction']\n",
    "    )]\n",
    "    \n",
    "    engineered_features = [f for f in best_features if f not in original_features]\n",
    "    \n",
    "    print(f\"\\nüìä Composition des features:\")\n",
    "    print(f\"  ‚Ä¢ Features originales: {len(original_features)}\")\n",
    "    print(f\"  ‚Ä¢ Features engineered: {len(engineered_features)}\")\n",
    "    \n",
    "    if engineered_features:\n",
    "        print(f\"  üìà Types de features cr√©√©es:\")\n",
    "        feature_types = {\n",
    "            'rolling': len([f for f in engineered_features if 'rolling' in f]),\n",
    "            'lag': len([f for f in engineered_features if 'lag' in f]),\n",
    "            'ratio': len([f for f in engineered_features if 'ratio' in f]),\n",
    "            'interaction': len([f for f in engineered_features if 'interaction' in f]),\n",
    "            'polynomial': len([f for f in engineered_features if any(\n",
    "                suffix in f for suffix in ['squared', 'log']\n",
    "            )])\n",
    "        }\n",
    "        \n",
    "        for ftype, count in feature_types.items():\n",
    "            if count > 0:\n",
    "                print(f\"    - {ftype}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüí° Conseils d'utilisation:\")\n",
    "    print(\"  1. Utiliser ces features comme point de d√©part\")\n",
    "    print(\"  2. Tester diff√©rentes combinaisons selon le mod√®le\")\n",
    "    print(\"  3. Surveiller l'overfitting avec features engineered\")\n",
    "    print(\"  4. Consid√©rer la stabilit√© temporelle des features\")\n",
    "    \n",
    "    if len(best_features) > 20:\n",
    "        print(\"  5. Envisager une r√©duction dimensionnelle suppl√©mentaire\")\n",
    "\n",
    "# G√©n√©rer les recommandations\n",
    "if 'formation_best_features' in locals():\n",
    "    generate_feature_recommendations(formation_engineered, formation_best_features, 'FormationPressure')\n",
    "\n",
    "if 'rop_best_features' in locals():\n",
    "    generate_feature_recommendations(formation_engineered, rop_best_features, 'ROP')\n",
    "\n",
    "if 'kick_best_features' in locals():\n",
    "    generate_feature_recommendations(kick_engineered, kick_best_features, 'Kick Detection')\n",
    "\n",
    "# ====================================================================\n",
    "# 7. SAUVEGARDE DES DONN√âES PREPROCESS√âES\n",
    "# ====================================================================\n",
    "\n",
    "print(f\"\\nüíæ SAUVEGARDE DES DONN√âES PREPROCESS√âES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Cr√©er les datasets finaux avec les meilleures features\n",
    "def create_final_datasets():\n",
    "    \"\"\"Cr√©e les datasets finaux pour la mod√©lisation\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    if 'formation_best_features' in locals() and formation_best_features:\n",
    "        # Dataset pour Formation Pressure\n",
    "        formation_final_features = formation_best_features + ['FormationPressure']\n",
    "        formation_final = formation_engineered[formation_final_features].copy()\n",
    "        datasets['formation_pressure'] = formation_final\n",
    "        print(f\"‚úÖ Dataset Formation Pressure: {formation_final.shape}\")\n",
    "    \n",
    "    if 'rop_best_features' in locals() and rop_best_features:\n",
    "        # Dataset pour ROP\n",
    "        rop_final_features = rop_best_features + ['ROP']\n",
    "        rop_final = formation_engineered[rop_final_features].copy()\n",
    "        datasets['rop_prediction'] = rop_final\n",
    "        print(f\"‚úÖ Dataset ROP Prediction: {rop_final.shape}\")\n",
    "    \n",
    "    if 'kick_best_features' in locals() and kick_best_features:\n",
    "        # Dataset pour Kick Detection\n",
    "        kick_final_features = kick_best_features + ['Kick']\n",
    "        kick_final = kick_engineered[kick_final_features].copy()\n",
    "        datasets['kick_detection'] = kick_final\n",
    "        print(f\"‚úÖ Dataset Kick Detection: {kick_final.shape}\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "final_datasets = create_final_datasets()\n",
    "\n",
    "# Sauvegarder les datasets\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "for name, dataset in final_datasets.items():\n",
    "    filepath = f'../data/processed/{name}_features.csv'\n",
    "    dataset.to_csv(filepath, index=False)\n",
    "    print(f\"üíæ Sauvegard√©: {filepath}\")\n",
    "\n",
    "# Sauvegarder les listes de features s√©lectionn√©es\n",
    "feature_lists = {}\n",
    "if 'formation_best_features' in locals():\n",
    "    feature_lists['formation_pressure'] = formation_best_features\n",
    "if 'rop_best_features' in locals():\n",
    "    feature_lists['rop_prediction'] = rop_best_features\n",
    "if 'kick_best_features' in locals():\n",
    "    feature_lists['kick_detection'] = kick_best_features\n",
    "\n",
    "import json\n",
    "with open('../data/processed/selected_features.json', 'w') as f:\n",
    "    json.dump(feature_lists, f, indent=2)\n",
    "print(\"üíæ Sauvegard√©: ../data/processed/selected_features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa7ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 8. R√âSUM√â ET CONCLUSIONS\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26fca553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã R√âSUM√â DE L'ANALYSE DES FEATURES\n",
      "============================================================\n",
      "üîç TRAVAIL R√âALIS√â:\n",
      "‚úÖ Analyse de l'importance des features existantes\n",
      "‚úÖ Cr√©ation de nouvelles features via feature engineering\n",
      "‚úÖ S√©lection des meilleures features par m√©thodes multiples\n",
      "‚úÖ Analyse dimensionnelle avec PCA\n",
      "‚úÖ D√©tection de multicolin√©arit√©\n",
      "‚úÖ Pr√©paration des datasets pour mod√©lisation\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'formation_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Pr√©paration des datasets pour mod√©lisation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Statistiques finales\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m total_original_features = \u001b[38;5;28mlen\u001b[39m([col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mformation_df\u001b[49m.columns \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_numeric_dtype(formation_df[col])])\n\u001b[32m     14\u001b[39m total_engineered_features = \u001b[38;5;28mlen\u001b[39m([col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m formation_engineered.columns \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_numeric_dtype(formation_engineered[col])])\n\u001b[32m     15\u001b[39m features_created = total_engineered_features - total_original_features\n",
      "\u001b[31mNameError\u001b[39m: name 'formation_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìã R√âSUM√â DE L'ANALYSE DES FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üîç TRAVAIL R√âALIS√â:\")\n",
    "print(\"‚úÖ Analyse de l'importance des features existantes\")\n",
    "print(\"‚úÖ Cr√©ation de nouvelles features via feature engineering\")\n",
    "print(\"‚úÖ S√©lection des meilleures features par m√©thodes multiples\")\n",
    "print(\"‚úÖ Analyse dimensionnelle avec PCA\")\n",
    "print(\"‚úÖ D√©tection de multicolin√©arit√©\")\n",
    "print(\"‚úÖ Pr√©paration des datasets pour mod√©lisation\")\n",
    "\n",
    "# Statistiques finales\n",
    "total_original_features = len([col for col in formation_df.columns if pd.api.types.is_numeric_dtype(formation_df[col])])\n",
    "total_engineered_features = len([col for col in formation_engineered.columns if pd.api.types.is_numeric_dtype(formation_engineered[col])])\n",
    "features_created = total_engineered_features - total_original_features\n",
    "\n",
    "print(f\"\\nüìä STATISTIQUES:\")\n",
    "print(f\"  ‚Ä¢ Features originales: {total_original_features}\")\n",
    "print(f\"  ‚Ä¢ Features cr√©√©es: {features_created}\")\n",
    "print(f\"  ‚Ä¢ Total apr√®s engineering: {total_engineered_features}\")\n",
    "\n",
    "if final_datasets:\n",
    "    print(f\"  ‚Ä¢ Datasets finaux cr√©√©s: {len(final_datasets)}\")\n",
    "    for name, dataset in final_datasets.items():\n",
    "        print(f\"    - {name}: {dataset.shape[1]-1} features + 1 target\")\n",
    "\n",
    "print(f\"\\nüéØ FEATURES LES PLUS IMPORTANTES:\")\n",
    "if 'formation_best_features' in locals():\n",
    "    print(f\"  Formation Pressure (Top 5): {formation_best_features[:5]}\")\n",
    "if 'rop_best_features' in locals():\n",
    "    print(f\"  ROP Prediction (Top 5): {rop_best_features[:5]}\")\n",
    "if 'kick_best_features' in locals():\n",
    "    print(f\"  Kick Detection (Top 5): {kick_best_features[:5]}\")\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS CL√âS:\")\n",
    "print(\"  ‚Ä¢ Les features engineered am√©liorent significativement la pr√©diction\")\n",
    "print(\"  ‚Ä¢ Les moyennes mobiles et ratios sont particuli√®rement informatives\")\n",
    "print(\"  ‚Ä¢ Attention √† la multicolin√©arit√© avec les features d√©riv√©es\")\n",
    "print(\"  ‚Ä¢ Les features temporelles ajoutent de la valeur pr√©dictive\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è POINTS D'ATTENTION:\")\n",
    "print(\"  ‚Ä¢ Valider la stabilit√© des features dans le temps\")\n",
    "print(\"  ‚Ä¢ Surveiller l'overfitting avec trop de features engineered\")\n",
    "print(\"  ‚Ä¢ Tester la robustesse des features sur donn√©es r√©elles\")\n",
    "print(\"  ‚Ä¢ Consid√©rer l'interpr√©tabilit√© m√©tier des features complexes\")\n",
    "\n",
    "print(f\"\\nüöÄ PROCHAINES √âTAPES:\")\n",
    "print(\"  1. Utiliser ces datasets dans le notebook 03_model_comparison.ipynb\")\n",
    "print(\"  2. Tester diff√©rents algorithmes de ML\")\n",
    "print(\"  3. Valider les performances sur donn√©es de test\")\n",
    "print(\"  4. Optimiser les hyperparam√®tres\")\n",
    "print(\"  5. Analyser l'importance des features dans les mod√®les finaux\")\n",
    "\n",
    "print(f\"\\nüìÅ FICHIERS CR√â√âS:\")\n",
    "for name in final_datasets.keys():\n",
    "    print(f\"  ‚Ä¢ ../data/processed/{name}_features.csv\")\n",
    "print(\"  ‚Ä¢ ../data/processed/selected_features.json\")\n",
    "\n",
    "print(f\"\\nüéâ ANALYSE DES FEATURES TERMIN√âE!\")\n",
    "print(\"Passez au notebook suivant: 03_model_comparison.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
