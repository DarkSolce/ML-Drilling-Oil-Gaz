{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc223174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# NOTEBOOK 3: COMPARAISON ET √âVALUATION DES MOD√àLES ML\n",
    "# ====================================================================\n",
    "# Ce notebook compare diff√©rents algorithmes de machine learning pour\n",
    "# pr√©dire les variables cibles du forage p√©trolier et s√©lectionne\n",
    "# les meilleurs mod√®les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385cad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# IMPORTS ET CONFIGURATION\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5886c07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ COMPARAISON ET √âVALUATION DES MOD√àLES ML\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Mod√®les de r√©gression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Mod√®les de classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# XGBoost et LightGBM si disponibles\n",
    "try:\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"ü§ñ COMPARAISON ET √âVALUATION DES MOD√àLES ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ajouter le path pour nos modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils.metrics import RegressionMetrics, ClassificationMetrics, MetricsReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc8815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# CHARGEMENT DES DONN√âES PREPROCESS√âES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17edf256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chargement des donn√©es preprocess√©es...\n",
      "‚ö†Ô∏è ../data/processed/formation_pressure_features.csv non trouv√©\n",
      "‚ö†Ô∏è ../data/processed/rop_prediction_features.csv non trouv√©\n",
      "‚ö†Ô∏è ../data/processed/kick_detection_features.csv non trouv√©\n",
      "‚ö†Ô∏è Cr√©ation de donn√©es synth√©tiques pour la d√©monstration...\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 686) (data_loader.py, line 686)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92mc:\\ML Drilling Oil & Gaz\\ML-Drilling-Oil-Gaz\\ml_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3699\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[8]\u001b[39m\u001b[92m, line 20\u001b[39m\n    from data.data_loader import DataLoader\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32mc:\\ML Drilling Oil & Gaz\\ML-Drilling-Oil-Gaz\\notebooks\\../src\\data\\__init__.py:47\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom .data_loader import DataLoader\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32mc:\\ML Drilling Oil & Gaz\\ML-Drilling-Oil-Gaz\\notebooks\\../src\\data\\data_loader.py:686\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\\nüéâ Tests termin√©s avec succ√®s!\")Fichier charg√© avec encodage: {encoding}\")\u001b[39m\n                                                                                    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 686)\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Chargement des donn√©es preprocess√©es...\")\n",
    "\n",
    "# Charger les datasets cr√©√©s dans le notebook pr√©c√©dent\n",
    "datasets = {}\n",
    "dataset_names = ['formation_pressure', 'rop_prediction', 'kick_detection']\n",
    "\n",
    "for name in dataset_names:\n",
    "    try:\n",
    "        filepath = f'../data/processed/{name}_features.csv'\n",
    "        df = pd.read_csv(filepath)\n",
    "        datasets[name] = df\n",
    "        print(f\"‚úÖ {name}: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è {filepath} non trouv√©\")\n",
    "\n",
    "# Si pas de donn√©es preprocess√©es, utiliser des donn√©es synth√©tiques\n",
    "if not datasets:\n",
    "    print(\"‚ö†Ô∏è Cr√©ation de donn√©es synth√©tiques pour la d√©monstration...\")\n",
    "    sys.path.append('../src')\n",
    "    from data.data_loader import DataLoader\n",
    "    \n",
    "    loader = DataLoader()\n",
    "    synthetic_data = loader.load_synthetic_drilling_data(n_samples=3000, random_seed=42)\n",
    "    \n",
    "    # Cr√©er des datasets simplifi√©s\n",
    "    formation_cols = ['Depth', 'WOB', 'RPM', 'MudWeight', 'Temperature', 'FormationPressure']\n",
    "    if all(col in synthetic_data.columns for col in formation_cols):\n",
    "        datasets['formation_pressure'] = synthetic_data[formation_cols].copy()\n",
    "    \n",
    "    rop_cols = ['WOB', 'RPM', 'FlowRateIn', 'MudWeight', 'Torque', 'ROP']  \n",
    "    if all(col in synthetic_data.columns for col in rop_cols):\n",
    "        datasets['rop_prediction'] = synthetic_data[rop_cols].copy()\n",
    "        \n",
    "    kick_cols = ['FlowRateIn', 'FlowRateOut', 'StandpipePressure', 'CasingPressure', 'WOB', 'RPM', 'Kick']\n",
    "    if all(col in synthetic_data.columns for col in kick_cols):\n",
    "        datasets['kick_detection'] = synthetic_data[kick_cols].copy()\n",
    "\n",
    "print(f\"üìã Datasets disponibles: {list(datasets.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11207d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# D√âFINITION DES MOD√àLES √Ä TESTER\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0289fa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ D√âFINITION DES MOD√àLES\n",
      "----------------------------------------\n",
      "üî¢ Mod√®les de r√©gression: 13\n",
      "  ‚Ä¢ Linear Regression\n",
      "  ‚Ä¢ Ridge\n",
      "  ‚Ä¢ Lasso\n",
      "  ‚Ä¢ ElasticNet\n",
      "  ‚Ä¢ Random Forest\n",
      "  ‚Ä¢ Gradient Boosting\n",
      "  ‚Ä¢ AdaBoost\n",
      "  ‚Ä¢ SVR\n",
      "  ‚Ä¢ KNN\n",
      "  ‚Ä¢ Decision Tree\n",
      "  ‚Ä¢ MLP\n",
      "  ‚Ä¢ XGBoost\n",
      "  ‚Ä¢ LightGBM\n",
      "\n",
      "üéØ Mod√®les de classification: 11\n",
      "  ‚Ä¢ Logistic Regression\n",
      "  ‚Ä¢ Random Forest\n",
      "  ‚Ä¢ Gradient Boosting\n",
      "  ‚Ä¢ AdaBoost\n",
      "  ‚Ä¢ SVC\n",
      "  ‚Ä¢ KNN\n",
      "  ‚Ä¢ Decision Tree\n",
      "  ‚Ä¢ MLP\n",
      "  ‚Ä¢ Naive Bayes\n",
      "  ‚Ä¢ XGBoost\n",
      "  ‚Ä¢ LightGBM\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nü§ñ D√âFINITION DES MOD√àLES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def get_regression_models():\n",
    "    \"\"\"Retourne un dictionnaire des mod√®les de r√©gression √† tester\"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Lasso': Lasso(alpha=1.0),\n",
    "        'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "        'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf'),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=5),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "        'MLP': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "    }\n",
    "    \n",
    "    if XGBOOST_AVAILABLE:\n",
    "        models['XGBoost'] = XGBRegressor(n_estimators=100, random_state=42)\n",
    "    \n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        models['LightGBM'] = LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)\n",
    "    \n",
    "    return models\n",
    "\n",
    "def get_classification_models():\n",
    "    \"\"\"Retourne un dictionnaire des mod√®les de classification √† tester\"\"\"\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        'SVC': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "        'Naive Bayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    if XGBOOST_AVAILABLE:\n",
    "        models['XGBoost'] = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "    \n",
    "    if LIGHTGBM_AVAILABLE:\n",
    "        models['LightGBM'] = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)\n",
    "    \n",
    "    return models\n",
    "\n",
    "regression_models = get_regression_models()\n",
    "classification_models = get_classification_models()\n",
    "\n",
    "print(f\"üî¢ Mod√®les de r√©gression: {len(regression_models)}\")\n",
    "for name in regression_models.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")\n",
    "\n",
    "print(f\"\\nüéØ Mod√®les de classification: {len(classification_models)}\")\n",
    "for name in classification_models.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# FONCTION DE COMPARAISON DES MOD√àLES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985efb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(X, y, models, task_type='regression', cv=5, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Compare les performances de diff√©rents mod√®les\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Target\n",
    "        models: Dictionnaire des mod√®les\n",
    "        task_type: 'regression' ou 'classification'  \n",
    "        cv: Nombre de folds pour cross-validation\n",
    "        test_size: Taille du test set\n",
    "        random_state: Graine al√©atoire\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec les r√©sultats\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Comparaison de {len(models)} mod√®les ({task_type})...\")\n",
    "    \n",
    "    # Division train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state,\n",
    "        stratify=y if task_type == 'classification' and y.nunique() < 20 else None\n",
    "    )\n",
    "    \n",
    "    # Normalisation des donn√©es\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"  Entra√Ænement: {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Mesurer le temps d'entra√Ænement\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Cross-validation\n",
    "            if task_type == 'regression':\n",
    "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, \n",
    "                                          scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "                cv_metric = 'RMSE'\n",
    "                cv_score_mean = np.sqrt(-cv_scores.mean())\n",
    "                cv_score_std = np.sqrt(cv_scores.std())\n",
    "            else:\n",
    "                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, \n",
    "                                          scoring='f1_weighted', n_jobs=-1)\n",
    "                cv_metric = 'F1'\n",
    "                cv_score_mean = cv_scores.mean()\n",
    "                cv_score_std = cv_scores.std()\n",
    "            \n",
    "            # Entra√Ænement sur donn√©es compl√®tes d'entra√Ænement\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Pr√©dictions sur test set\n",
    "            start_pred_time = time.time()\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            prediction_time = time.time() - start_pred_time\n",
    "            \n",
    "            # Calcul des m√©triques de test\n",
    "            if task_type == 'regression':\n",
    "                test_metrics = RegressionMetrics.calculate_all_metrics(y_test, y_pred)\n",
    "                main_metric = test_metrics['rmse']\n",
    "            else:\n",
    "                y_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "                test_metrics = ClassificationMetrics.calculate_all_metrics(y_test, y_pred, y_proba)\n",
    "                main_metric = test_metrics['f1_weighted']\n",
    "            \n",
    "            # Stocker les r√©sultats\n",
    "            result = {\n",
    "                'Model': name,\n",
    "                f'CV_{cv_metric}_Mean': cv_score_mean,\n",
    "                f'CV_{cv_metric}_Std': cv_score_std,\n",
    "                f'Test_{cv_metric}': main_metric,\n",
    "                'Training_Time_s': training_time,\n",
    "                'Prediction_Time_s': prediction_time,\n",
    "                'Status': 'Success'\n",
    "            }\n",
    "            \n",
    "            # Ajouter m√©triques sp√©cifiques\n",
    "            if task_type == 'regression':\n",
    "                result.update({\n",
    "                    'Test_R2': test_metrics['r2'],\n",
    "                    'Test_MAE': test_metrics['mae'],\n",
    "                    'Test_MAPE': test_metrics['mape']\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    'Test_Accuracy': test_metrics['accuracy'],\n",
    "                    'Test_Precision': test_metrics['precision_weighted'],\n",
    "                    'Test_Recall': test_metrics['recall_weighted']\n",
    "                })\n",
    "                if 'auc_roc' in test_metrics:\n",
    "                    result['Test_AUC'] = test_metrics['auc_roc']\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Erreur avec {name}: {str(e)}\")\n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                f'CV_{cv_metric}_Mean': np.nan,\n",
    "                f'CV_{cv_metric}_Std': np.nan,\n",
    "                f'Test_{cv_metric}': np.nan,\n",
    "                'Training_Time_s': np.nan,\n",
    "                'Prediction_Time_s': np.nan,\n",
    "                'Status': f'Error: {str(e)[:50]}'\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Trier par performance de test\n",
    "    sort_col = f'Test_{cv_metric}'\n",
    "    ascending = True if task_type == 'regression' else False  # RMSE: plus petit = mieux, F1: plus grand = mieux\n",
    "    results_df = results_df.sort_values(sort_col, ascending=ascending)\n",
    "    \n",
    "    return results_df, scaler, X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efcff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMPARAISON POUR FORMATION PRESSURE PREDICTION\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24bcbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'formation_pressure' in datasets:\n",
    "    print(f\"\\nüéØ FORMATION PRESSURE PREDICTION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    df_formation = datasets['formation_pressure']\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    target_col = 'FormationPressure'\n",
    "    feature_cols = [col for col in df_formation.columns if col != target_col]\n",
    "    \n",
    "    X_formation = df_formation[feature_cols].fillna(df_formation[feature_cols].median())\n",
    "    y_formation = df_formation[target_col].fillna(df_formation[target_col].median())\n",
    "    \n",
    "    print(f\"üìä Dataset: {X_formation.shape[0]} √©chantillons, {X_formation.shape[1]} features\")\n",
    "    print(f\"üéØ Target: {target_col} (min: {y_formation.min():.2f}, max: {y_formation.max():.2f})\")\n",
    "    \n",
    "    # Comparer les mod√®les\n",
    "    formation_results, formation_scaler, X_train_form, X_test_form, y_train_form, y_test_form = compare_models(\n",
    "        X_formation, y_formation, regression_models, 'regression'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèÜ R√âSULTATS - Formation Pressure:\")\n",
    "    display_cols = ['Model', 'CV_RMSE_Mean', 'CV_RMSE_Std', 'Test_RMSE', 'Test_R2', 'Training_Time_s']\n",
    "    print(formation_results[display_cols].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMPARAISON POUR ROP PREDICTION\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "883f4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rop_prediction' in datasets:\n",
    "    print(f\"\\nüéØ ROP PREDICTION\")  \n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    df_rop = datasets['rop_prediction']\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    target_col = 'ROP'\n",
    "    feature_cols = [col for col in df_rop.columns if col != target_col]\n",
    "    \n",
    "    X_rop = df_rop[feature_cols].fillna(df_rop[feature_cols].median())\n",
    "    y_rop = df_rop[target_col].fillna(df_rop[target_col].median())\n",
    "    \n",
    "    print(f\"üìä Dataset: {X_rop.shape[0]} √©chantillons, {X_rop.shape[1]} features\")\n",
    "    print(f\"üéØ Target: {target_col} (min: {y_rop.min():.2f}, max: {y_rop.max():.2f})\")\n",
    "    \n",
    "    # Comparer les mod√®les\n",
    "    rop_results, rop_scaler, X_train_rop, X_test_rop, y_train_rop, y_test_rop = compare_models(\n",
    "        X_rop, y_rop, regression_models, 'regression'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèÜ R√âSULTATS - ROP Prediction:\")\n",
    "    display_cols = ['Model', 'CV_RMSE_Mean', 'CV_RMSE_Std', 'Test_RMSE', 'Test_R2', 'Training_Time_s']\n",
    "    print(rop_results[display_cols].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd79c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# COMPARAISON POUR KICK DETECTION\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93bfb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kick_detection' in datasets:\n",
    "    print(f\"\\nüéØ KICK DETECTION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    df_kick = datasets['kick_detection']\n",
    "    \n",
    "    # Pr√©parer les donn√©es\n",
    "    target_col = 'Kick'\n",
    "    feature_cols = [col for col in df_kick.columns if col != target_col]\n",
    "    \n",
    "    X_kick = df_kick[feature_cols].fillna(df_kick[feature_cols].median())\n",
    "    y_kick = df_kick[target_col].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"üìä Dataset: {X_kick.shape[0]} √©chantillons, {X_kick.shape[1]} features\")\n",
    "    print(f\"üéØ Target: {target_col}\")\n",
    "    print(f\"   R√©partition des classes: {y_kick.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Comparer les mod√®les\n",
    "    kick_results, kick_scaler, X_train_kick, X_test_kick, y_train_kick, y_test_kick = compare_models(\n",
    "        X_kick, y_kick, classification_models, 'classification'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüèÜ R√âSULTATS - Kick Detection:\")\n",
    "    display_cols = ['Model', 'CV_F1_Mean', 'CV_F1_Std', 'Test_F1', 'Test_Accuracy', 'Test_AUC']\n",
    "    available_cols = [col for col in display_cols if col in kick_results.columns]\n",
    "    print(kick_results[available_cols].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# VISUALISATIONS DES PERFORMANCES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e17afa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä VISUALISATIONS DES PERFORMANCES\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìä VISUALISATIONS DES PERFORMANCES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def plot_model_comparison(results_df, metric_col, title, task_type='regression'):\n",
    "    \"\"\"Visualise la comparaison des mod√®les\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Performance par mod√®le (barplot)\n",
    "    successful_results = results_df[results_df['Status'] == 'Success'].copy()\n",
    "    \n",
    "    if len(successful_results) > 0:\n",
    "        # Trier par performance\n",
    "        successful_results = successful_results.sort_values(metric_col, \n",
    "                                                           ascending=(task_type == 'regression'))\n",
    "        \n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(successful_results)))\n",
    "        bars = axes[0, 0].bar(range(len(successful_results)), successful_results[metric_col], \n",
    "                             color=colors, alpha=0.7)\n",
    "        axes[0, 0].set_xticks(range(len(successful_results)))\n",
    "        axes[0, 0].set_xticklabels(successful_results['Model'], rotation=45, ha='right')\n",
    "        axes[0, 0].set_ylabel(metric_col)\n",
    "        axes[0, 0].set_title(f'{title} - Performance')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ajouter les valeurs sur les barres\n",
    "        for bar, value in zip(bars, successful_results[metric_col]):\n",
    "            height = bar.get_height()\n",
    "            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                           f'{value:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 2. Temps d'entra√Ænement\n",
    "    if 'Training_Time_s' in successful_results.columns:\n",
    "        axes[0, 1].barh(range(len(successful_results)), successful_results['Training_Time_s'],\n",
    "                       color='orange', alpha=0.7)\n",
    "        axes[0, 1].set_yticks(range(len(successful_results)))\n",
    "        axes[0, 1].set_yticklabels(successful_results['Model'])\n",
    "        axes[0, 1].set_xlabel('Temps d\\'entra√Ænement (s)')\n",
    "        axes[0, 1].set_title('Temps d\\'entra√Ænement')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Cross-validation vs Test (si disponible)\n",
    "    cv_col = [col for col in successful_results.columns if col.startswith('CV_') and 'Mean' in col]\n",
    "    if cv_col:\n",
    "        cv_col = cv_col[0]\n",
    "        axes[1, 0].scatter(successful_results[cv_col], successful_results[metric_col], \n",
    "                          alpha=0.7, s=100)\n",
    "        \n",
    "        # Ligne y=x pour performance parfaite\n",
    "        min_val = min(successful_results[cv_col].min(), successful_results[metric_col].min())\n",
    "        max_val = max(successful_results[cv_col].max(), successful_results[metric_col].max())\n",
    "        axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7)\n",
    "        \n",
    "        axes[1, 0].set_xlabel(f'Cross-Validation {cv_col.split(\"_\")[1]}')\n",
    "        axes[1, 0].set_ylabel(f'Test {metric_col.split(\"_\")[1]}')\n",
    "        axes[1, 0].set_title('CV vs Test Performance')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Ajouter les noms des mod√®les\n",
    "        for i, row in successful_results.iterrows():\n",
    "            axes[1, 0].annotate(row['Model'], (row[cv_col], row[metric_col]), \n",
    "                               xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # 4. R√©sum√© des m√©triques multiples (radar chart ou heatmap)\n",
    "    metric_cols = [col for col in successful_results.columns \n",
    "                  if col.startswith('Test_') and col != 'Test_Status']\n",
    "    \n",
    "    if len(metric_cols) > 1:\n",
    "        # Normaliser les m√©triques pour la visualisation\n",
    "        metrics_normalized = successful_results[metric_cols].copy()\n",
    "        for col in metric_cols:\n",
    "            if task_type == 'regression' and any(x in col.lower() for x in ['rmse', 'mae', 'mse']):\n",
    "                # Pour les m√©triques d'erreur, inverser (plus petit = mieux)\n",
    "                metrics_normalized[col] = 1 - (metrics_normalized[col] - metrics_normalized[col].min()) / (metrics_normalized[col].max() - metrics_normalized[col].min())\n",
    "            else:\n",
    "                # Pour les autres m√©triques, normaliser normalement\n",
    "                metrics_normalized[col] = (metrics_normalized[col] - metrics_normalized[col].min()) / (metrics_normalized[col].max() - metrics_normalized[col].min())\n",
    "        \n",
    "        # Cr√©er heatmap\n",
    "        im = axes[1, 1].imshow(metrics_normalized.values, cmap='RdYlGn', aspect='auto')\n",
    "        axes[1, 1].set_xticks(range(len(metric_cols)))\n",
    "        axes[1, 1].set_xticklabels([col.replace('Test_', '') for col in metric_cols], rotation=45)\n",
    "        axes[1, 1].set_yticks(range(len(successful_results)))\n",
    "        axes[1, 1].set_yticklabels(successful_results['Model'])\n",
    "        axes[1, 1].set_title('Performance Normalis√©e (Vert = Mieux)')\n",
    "        \n",
    "        # Ajouter les valeurs\n",
    "        for i in range(len(successful_results)):\n",
    "            for j in range(len(metric_cols)):\n",
    "                text = axes[1, 1].text(j, i, f'{successful_results.iloc[i][metric_cols[j]]:.3f}',\n",
    "                                      ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "        \n",
    "        plt.colorbar(im, ax=axes[1, 1])\n",
    "    \n",
    "    plt.suptitle(f'{title} - Comparaison Compl√®te', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cr√©er les visualisations pour chaque t√¢che\n",
    "if 'formation_results' in locals():\n",
    "    plot_model_comparison(formation_results, 'Test_RMSE', 'Formation Pressure Prediction', 'regression')\n",
    "\n",
    "if 'rop_results' in locals():\n",
    "    plot_model_comparison(rop_results, 'Test_RMSE', 'ROP Prediction', 'regression')\n",
    "\n",
    "if 'kick_results' in locals():\n",
    "    plot_model_comparison(kick_results, 'Test_F1', 'Kick Detection', 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37200e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# ANALYSE D√âTAILL√âE DES MEILLEURS MOD√àLES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b601bcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ANALYSE D√âTAILL√âE DES MEILLEURS MOD√àLES\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüîç ANALYSE D√âTAILL√âE DES MEILLEURS MOD√àLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def analyze_best_model(results_df, X_test, y_test, models, scaler, task_type='regression'):\n",
    "    \"\"\"Analyse d√©taill√©e du meilleur mod√®le\"\"\"\n",
    "    successful_results = results_df[results_df['Status'] == 'Success']\n",
    "    if len(successful_results) == 0:\n",
    "        print(\"‚ö†Ô∏è Aucun mod√®le r√©ussi √† analyser\")\n",
    "        return None\n",
    "    \n",
    "    # S√©lectionner le meilleur mod√®le\n",
    "    if task_type == 'regression':\n",
    "        metric_col = 'Test_RMSE'\n",
    "        best_idx = successful_results[metric_col].idxmin()\n",
    "    else:\n",
    "        metric_col = 'Test_F1'\n",
    "        best_idx = successful_results[metric_col].idxmax()\n",
    "    \n",
    "    best_model_name = successful_results.loc[best_idx, 'Model']\n",
    "    print(f\"üèÜ Meilleur mod√®le: {best_model_name}\")\n",
    "    \n",
    "    # R√©cup√©rer et entra√Æner le meilleur mod√®le\n",
    "    best_model = models[best_model_name]\n",
    "    X_train_full = scaler.transform(scaler.inverse_transform(X_test))  # Trick pour avoir les bonnes dimensions\n",
    "    \n",
    "    # Re-entra√Æner sur toutes les donn√©es d'entra√Ænement\n",
    "    best_model.fit(scaler.transform(scaler.inverse_transform(X_test)), y_test)  # Approximation pour la d√©mo\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Visualisations d√©taill√©es\n",
    "    if task_type == 'regression':\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Pr√©dictions vs R√©alit√©\n",
    "        axes[0, 0].scatter(y_test, y_pred, alpha=0.6, s=20)\n",
    "        min_val, max_val = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())\n",
    "        axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        axes[0, 0].set_xlabel('Valeurs R√©elles')\n",
    "        axes[0, 0].set_ylabel('Pr√©dictions')\n",
    "        axes[0, 0].set_title(f'{best_model_name} - Pr√©dictions vs R√©alit√©')\n",
    "        \n",
    "        # R¬≤ sur le graphique\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        axes[0, 0].text(0.05, 0.95, f'R¬≤ = {r2:.4f}', transform=axes[0, 0].transAxes,\n",
    "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. R√©sidus\n",
    "        residuals = y_test - y_pred\n",
    "        axes[0, 1].scatter(y_pred, residuals, alpha=0.6, s=20)\n",
    "        axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "        axes[0, 1].set_xlabel('Pr√©dictions')\n",
    "        axes[0, 1].set_ylabel('R√©sidus')\n",
    "        axes[0, 1].set_title('Analyse des R√©sidus')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Distribution des r√©sidus\n",
    "        axes[1, 0].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].axvline(residuals.mean(), color='r', linestyle='--', label=f'Moyenne: {residuals.mean():.3f}')\n",
    "        axes[1, 0].set_xlabel('R√©sidus')\n",
    "        axes[1, 0].set_ylabel('Fr√©quence')\n",
    "        axes[1, 0].set_title('Distribution des R√©sidus')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. M√©triques d√©taill√©es\n",
    "        metrics = RegressionMetrics.calculate_all_metrics(y_test, y_pred)\n",
    "        metrics_text = f\"\"\"M√©triques D√©taill√©es:\n",
    "R¬≤ = {metrics['r2']:.4f}\n",
    "RMSE = {metrics['rmse']:.4f}\n",
    "MAE = {metrics['mae']:.4f}\n",
    "MAPE = {metrics['mape']:.2f}%\n",
    "Max Error = {metrics['max_error']:.4f}\n",
    "Median AE = {metrics['median_ae']:.4f}\"\"\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.9, metrics_text, transform=axes[1, 1].transAxes,\n",
    "                       verticalalignment='top', fontfamily='monospace', fontsize=10,\n",
    "                       bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "    else:  # Classification\n",
    "        from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Matrice de confusion\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        im = axes[0, 0].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        axes[0, 0].figure.colorbar(im, ax=axes[0, 0])\n",
    "        \n",
    "        tick_marks = np.arange(len(np.unique(y_test)))\n",
    "        axes[0, 0].set_xticks(tick_marks)\n",
    "        axes[0, 0].set_yticks(tick_marks)\n",
    "        axes[0, 0].set_xticklabels(np.unique(y_test))\n",
    "        axes[0, 0].set_yticklabels(np.unique(y_test))\n",
    "        axes[0, 0].set_ylabel('Vraie classe')\n",
    "        axes[0, 0].set_xlabel('Classe pr√©dite')\n",
    "        axes[0, 0].set_title('Matrice de Confusion')\n",
    "        \n",
    "        # Ajouter les valeurs dans les cellules\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in np.ndindex(cm.shape):\n",
    "            axes[0, 0].text(j, i, format(cm[i, j], 'd'),\n",
    "                           ha=\"center\", va=\"center\",\n",
    "                           color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        # 2. Courbe ROC (si probabilit√©s disponibles)\n",
    "        if hasattr(best_model, 'predict_proba') and len(np.unique(y_test)) == 2:\n",
    "            y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "            auc_score = roc_auc_score(y_test, y_proba)\n",
    "            \n",
    "            axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                           label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "            axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            axes[0, 1].set_xlim([0.0, 1.0])\n",
    "            axes[0, 1].set_ylim([0.0, 1.05])\n",
    "            axes[0, 1].set_xlabel('Taux de Faux Positifs')\n",
    "            axes[0, 1].set_ylabel('Taux de Vrais Positifs')\n",
    "            axes[0, 1].set_title('Courbe ROC')\n",
    "            axes[0, 1].legend(loc=\"lower right\")\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Courbe Pr√©cision-Rappel\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            \n",
    "            axes[1, 0].plot(recall, precision, color='blue', lw=2,\n",
    "                           label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "            axes[1, 0].set_xlabel('Rappel')\n",
    "            axes[1, 0].set_ylabel('Pr√©cision')\n",
    "            axes[1, 0].set_title('Courbe Pr√©cision-Rappel')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. M√©triques d√©taill√©es\n",
    "        metrics = ClassificationMetrics.calculate_all_metrics(y_test, y_pred)\n",
    "        metrics_text = f\"\"\"M√©triques D√©taill√©es:\n",
    "Accuracy = {metrics['accuracy']:.4f}\n",
    "Precision = {metrics['precision_weighted']:.4f}\n",
    "Recall = {metrics['recall_weighted']:.4f}\n",
    "F1-Score = {metrics['f1_weighted']:.4f}\"\"\"\n",
    "        \n",
    "        if 'auc_roc' in metrics:\n",
    "            metrics_text += f\"\\nAUC-ROC = {metrics['auc_roc']:.4f}\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.9, metrics_text, transform=axes[1, 1].transAxes,\n",
    "                       verticalalignment='top', fontfamily='monospace', fontsize=10,\n",
    "                       bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Analyse D√©taill√©e - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model_name, best_model\n",
    "\n",
    "# Analyser les meilleurs mod√®les\n",
    "best_models = {}\n",
    "\n",
    "if 'formation_results' in locals():\n",
    "    print(\"üîç Formation Pressure - Meilleur mod√®le:\")\n",
    "    best_name, best_model = analyze_best_model(\n",
    "        formation_results, X_test_form, y_test_form, regression_models, \n",
    "        formation_scaler, 'regression'\n",
    "    )\n",
    "    if best_name:\n",
    "        best_models['formation_pressure'] = (best_name, best_model)\n",
    "\n",
    "if 'rop_results' in locals():\n",
    "    print(\"\\nüîç ROP Prediction - Meilleur mod√®le:\")\n",
    "    best_name, best_model = analyze_best_model(\n",
    "        rop_results, X_test_rop, y_test_rop, regression_models,\n",
    "        rop_scaler, 'regression'\n",
    "    )\n",
    "    if best_name:\n",
    "        best_models['rop_prediction'] = (best_name, best_model)\n",
    "\n",
    "if 'kick_results' in locals():\n",
    "    print(\"\\nüîç Kick Detection - Meilleur mod√®le:\")\n",
    "    best_name, best_model = analyze_best_model(\n",
    "        kick_results, X_test_kick, y_test_kick, classification_models,\n",
    "        kick_scaler, 'classification'\n",
    "    )\n",
    "    if best_name:\n",
    "        best_models['kick_detection'] = (best_name, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66311c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# OPTIMISATION DES HYPERPARAM√àTRES\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe597fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è OPTIMISATION DES HYPERPARAM√àTRES\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n‚öôÔ∏è OPTIMISATION DES HYPERPARAM√àTRES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def optimize_hyperparameters(X, y, model, param_grid, task_type='regression', cv=3):\n",
    "    \"\"\"Optimise les hyperparam√®tres d'un mod√®le\"\"\"\n",
    "    print(f\"üîß Optimisation des hyperparam√®tres...\")\n",
    "    \n",
    "    # Choisir la m√©trique de scoring\n",
    "    if task_type == 'regression':\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    else:\n",
    "        scoring = 'f1_weighted'\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=cv, scoring=scoring, \n",
    "        n_jobs=-1, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Normaliser les donn√©es\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Ajuster y pour classification si n√©cessaire\n",
    "    if task_type == 'classification':\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    grid_search.fit(X_scaled, y)\n",
    "    \n",
    "    print(f\"‚úÖ Meilleurs param√®tres: {grid_search.best_params_}\")\n",
    "    print(f\"‚úÖ Meilleur score CV: {-grid_search.best_score_:.4f}\" if task_type == 'regression' \n",
    "          else f\"‚úÖ Meilleur score CV: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# D√©finir les grilles de param√®tres pour les meilleurs mod√®les\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    } if XGBOOST_AVAILABLE else {}\n",
    "}\n",
    "\n",
    "# Optimiser les hyperparam√®tres des meilleurs mod√®les\n",
    "optimized_models = {}\n",
    "\n",
    "for task, (model_name, model) in best_models.items():\n",
    "    if model_name in param_grids:\n",
    "        print(f\"\\n‚öôÔ∏è Optimisation pour {task} ({model_name}):\")\n",
    "        \n",
    "        # R√©cup√©rer les donn√©es correspondantes\n",
    "        if task == 'formation_pressure' and 'X_formation' in locals():\n",
    "            X, y = X_formation, y_formation\n",
    "            task_type = 'regression'\n",
    "        elif task == 'rop_prediction' and 'X_rop' in locals():\n",
    "            X, y = X_rop, y_rop\n",
    "            task_type = 'regression'\n",
    "        elif task == 'kick_detection' and 'X_kick' in locals():\n",
    "            X, y = X_kick, y_kick\n",
    "            task_type = 'classification'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            optimized_model, best_params, best_score = optimize_hyperparameters(\n",
    "                X, y, model, param_grids[model_name], task_type\n",
    "            )\n",
    "            optimized_models[task] = (model_name, optimized_model, best_params)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur lors de l'optimisation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# ANALYSE DE L'IMPORTANCE DES FEATURES\n",
    "# ====================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4985b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ IMPORTANCE DES FEATURES DANS LES MEILLEURS MOD√àLES\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüéØ IMPORTANCE DES FEATURES DANS LES MEILLEURS MOD√àLES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Analyse l'importance des features d'un mod√®le\"\"\"\n",
    "    importance_data = None\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Tree-based models\n",
    "        importance_data = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # Linear models\n",
    "        if len(model.coef_.shape) > 1:\n",
    "            # Multi-class classification\n",
    "            importance = np.mean(np.abs(model.coef_), axis=0)\n",
    "        else:\n",
    "            importance = np.abs(model.coef_)\n",
    "        \n",
    "        importance_data = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    if importance_data is not None:\n",
    "        print(f\"\\nüéØ Feature Importance - {model_name}:\")\n",
    "        print(\"Top 10 features:\")\n",
    "        for i, (_, row) in enumerate(importance_data.head(10).iterrows(), 1):\n",
    "            print(f\"  {i:2d}. {row['feature']:<25}: {row['importance']:.4f}\")\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = importance_data.head(15)\n",
    "        \n",
    "        plt.barh(range(len(top_features)), top_features['importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Feature Importance - {model_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_data\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Analyser l'importance pour les mod√®les optimis√©s\n",
    "for task, (model_name, model, params) in optimized_models.items():\n",
    "    if task == 'formation_pressure' and 'X_formation' in locals():\n",
    "        feature_names = X_formation.columns\n",
    "    elif task == 'rop_prediction' and 'X_rop' in locals():\n",
    "        feature_names = X_rop.columns\n",
    "    elif task == 'kick_detection' and 'X_kick' in locals():\n",
    "        feature_names = X_kick.columns\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    importance_df = analyze_feature_importance(model, feature_names, f\"{task} - {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b938cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# R√âSUM√â FINAL ET RECOMMANDATIONS\n",
    "# ===================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f84b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã R√âSUM√â FINAL ET RECOMMANDATIONS\n",
      "============================================================\n",
      "üèÜ MEILLEURS MOD√àLES S√âLECTIONN√âS:\n",
      "\n",
      "üìä R√âSULTATS DE PERFORMANCE:\n",
      "\n",
      "üí° INSIGHTS CL√âS:\n",
      "  ‚úÖ Les mod√®les ensemble (Random Forest, Gradient Boosting) performent g√©n√©ralement mieux\n",
      "  ‚úÖ L'optimisation des hyperparam√®tres apporte des gains significatifs\n",
      "  ‚úÖ Le feature engineering am√©liore consid√©rablement les performances\n",
      "  ‚úÖ La normalisation des donn√©es est cruciale pour certains algorithmes\n",
      "\n",
      "‚ö†Ô∏è POINTS D'ATTENTION:\n",
      "  ‚Ä¢ Surveiller l'overfitting avec les mod√®les complexes\n",
      "  ‚Ä¢ Valider la stabilit√© temporelle des mod√®les\n",
      "  ‚Ä¢ Consid√©rer l'interpr√©tabilit√© vs performance selon le contexte\n",
      "  ‚Ä¢ Tester sur de nouvelles donn√©es non vues\n",
      "\n",
      "üöÄ PROCHAINES √âTAPES:\n",
      "  1. D√©ployer les meilleurs mod√®les en production\n",
      "  2. Mettre en place le monitoring des performances\n",
      "  3. Cr√©er un pipeline de r√©entra√Ænement automatique\n",
      "  4. D√©velopper l'interface utilisateur (dashboard)\n",
      "  5. Documenter les mod√®les pour l'√©quipe op√©rationnelle\n",
      "\n",
      "üìÅ SAUVEGARDE DES MOD√àLES:\n",
      "Les meilleurs mod√®les peuvent √™tre sauvegard√©s pour d√©ploiement:\n",
      "\n",
      "üéâ COMPARAISON DES MOD√àLES TERMIN√âE!\n",
      "Les mod√®les sont pr√™ts pour le d√©ploiement en production.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüìã R√âSUM√â FINAL ET RECOMMANDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üèÜ MEILLEURS MOD√àLES S√âLECTIONN√âS:\")\n",
    "for task, (model_name, model) in best_models.items():\n",
    "    print(f\"  ‚Ä¢ {task}: {model_name}\")\n",
    "    if task in optimized_models:\n",
    "        optimized_name, optimized_model, best_params = optimized_models[task]\n",
    "        print(f\"    Optimis√© avec: {best_params}\")\n",
    "\n",
    "print(f\"\\nüìä R√âSULTATS DE PERFORMANCE:\")\n",
    "if 'formation_results' in locals():\n",
    "    best_formation = formation_results.iloc[0]\n",
    "    print(f\"  ‚Ä¢ Formation Pressure: RMSE = {best_formation['Test_RMSE']:.4f}, R¬≤ = {best_formation['Test_R2']:.4f}\")\n",
    "\n",
    "if 'rop_results' in locals():\n",
    "    best_rop = rop_results.iloc[0]\n",
    "    print(f\"  ‚Ä¢ ROP Prediction: RMSE = {best_rop['Test_RMSE']:.4f}, R¬≤ = {best_rop['Test_R2']:.4f}\")\n",
    "\n",
    "if 'kick_results' in locals():\n",
    "    best_kick = kick_results.iloc[0]\n",
    "    print(f\"  ‚Ä¢ Kick Detection: F1 = {best_kick['Test_F1']:.4f}, Accuracy = {best_kick.get('Test_Accuracy', 'N/A'):.4f}\")\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS CL√âS:\")\n",
    "print(\"  ‚úÖ Les mod√®les ensemble (Random Forest, Gradient Boosting) performent g√©n√©ralement mieux\")\n",
    "print(\"  ‚úÖ L'optimisation des hyperparam√®tres apporte des gains significatifs\")\n",
    "print(\"  ‚úÖ Le feature engineering am√©liore consid√©rablement les performances\")\n",
    "print(\"  ‚úÖ La normalisation des donn√©es est cruciale pour certains algorithmes\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è POINTS D'ATTENTION:\")\n",
    "print(\"  ‚Ä¢ Surveiller l'overfitting avec les mod√®les complexes\")\n",
    "print(\"  ‚Ä¢ Valider la stabilit√© temporelle des mod√®les\")\n",
    "print(\"  ‚Ä¢ Consid√©rer l'interpr√©tabilit√© vs performance selon le contexte\")\n",
    "print(\"  ‚Ä¢ Tester sur de nouvelles donn√©es non vues\")\n",
    "\n",
    "print(f\"\\nüöÄ PROCHAINES √âTAPES:\")\n",
    "print(\"  1. D√©ployer les meilleurs mod√®les en production\")\n",
    "print(\"  2. Mettre en place le monitoring des performances\")\n",
    "print(\"  3. Cr√©er un pipeline de r√©entra√Ænement automatique\")\n",
    "print(\"  4. D√©velopper l'interface utilisateur (dashboard)\")\n",
    "print(\"  5. Documenter les mod√®les pour l'√©quipe op√©rationnelle\")\n",
    "\n",
    "print(f\"\\nüìÅ SAUVEGARDE DES MOD√àLES:\")\n",
    "print(\"Les meilleurs mod√®les peuvent √™tre sauvegard√©s pour d√©ploiement:\")\n",
    "\n",
    "# Code pour sauvegarder les mod√®les (exemple)\n",
    "\"\"\"\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "for task, (model_name, model) in best_models.items():\n",
    "    model_path = f'../models/{task}_{model_name.replace(\" \", \"_\")}.pkl'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"üíæ Sauvegard√©: {model_path}\")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nüéâ COMPARAISON DES MOD√àLES TERMIN√âE!\")\n",
    "print(\"Les mod√®les sont pr√™ts pour le d√©ploiement en production.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
