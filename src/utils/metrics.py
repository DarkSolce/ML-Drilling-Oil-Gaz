"""
Metrics Module for ML Drilling Project
======================================

Module contenant toutes les m√©triques personnalis√©es pour l'√©valuation 
des mod√®les de forage p√©trolier
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.metrics import precision_recall_curve, roc_curve, auc
import logging

logger = logging.getLogger(__name__)

class RegressionMetrics:
    """
    Classe pour calculer les m√©triques de r√©gression
    """
    
    @staticmethod
    def calculate_skewness(data: np.ndarray) -> float:
        """
        Calcule l'asym√©trie (skewness) des donn√©es
        
        Args:
            data: Donn√©es √† analyser
            
        Returns:
            Coefficient d'asym√©trie
        """
        from scipy.stats import skew
        return float(skew(data))
    
    @staticmethod
    def drilling_efficiency_score(y_true: np.ndarray, y_pred: np.ndarray, 
                                 target_rop: float = 20.0) -> float:
        """
        Score d'efficacit√© sp√©cifique au forage (ROP)
        
        Args:
            y_true: ROP r√©elles
            y_pred: ROP pr√©dites
            target_rop: ROP cible pour le calcul d'efficacit√©
            
        Returns:
            Score d'efficacit√© (0-1, 1 √©tant parfait)
        """
        # Calculer l'erreur relative par rapport √† la cible
        relative_error = np.abs(y_true - y_pred) / target_rop
        
        # Score bas√© sur l'inverse de l'erreur relative
        efficiency_score = np.mean(1 / (1 + relative_error))
        
        return float(efficiency_score)

class ClassificationMetrics:
    """
    Classe pour calculer les m√©triques de classification
    """
    
    @staticmethod
    def calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray, 
                             y_proba: Optional[np.ndarray] = None, 
                             labels: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Calcule toutes les m√©triques de classification
        
        Args:
            y_true: Valeurs r√©elles
            y_pred: Pr√©dictions
            y_proba: Probabilit√©s (optionnel)
            labels: Labels des classes (optionnel)
            
        Returns:
            Dictionnaire avec toutes les m√©triques
        """
        metrics = {}
        
        # M√©triques de base
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro', zero_division=0)
        metrics['precision_weighted'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)
        metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro', zero_division=0)
        metrics['recall_weighted'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)
        metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)
        metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)
        
        # Matrice de confusion
        cm = confusion_matrix(y_true, y_pred)
        metrics['confusion_matrix'] = cm.tolist()
        
        # M√©triques par classe
        if labels:
            class_report = classification_report(y_true, y_pred, target_names=labels, output_dict=True)
            metrics['classification_report'] = class_report
        
        # M√©triques bas√©es sur les probabilit√©s
        if y_proba is not None:
            try:
                if y_proba.ndim == 1 or y_proba.shape[1] == 2:
                    # Classification binaire
                    if y_proba.ndim == 2:
                        y_proba_binary = y_proba[:, 1]
                    else:
                        y_proba_binary = y_proba
                    
                    metrics['auc_roc'] = roc_auc_score(y_true, y_proba_binary)
                    
                    # Calculer la courbe ROC
                    fpr, tpr, _ = roc_curve(y_true, y_proba_binary)
                    metrics['roc_curve'] = {'fpr': fpr.tolist(), 'tpr': tpr.tolist()}
                    
                    # Courbe Pr√©cision-Rappel
                    precision, recall, _ = precision_recall_curve(y_true, y_proba_binary)
                    metrics['pr_auc'] = auc(recall, precision)
                    metrics['pr_curve'] = {'precision': precision.tolist(), 'recall': recall.tolist()}
                
                else:
                    # Classification multi-classes
                    metrics['auc_roc'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')
                    
            except Exception as e:
                logger.warning(f"Erreur lors du calcul des m√©triques probabilistes: {e}")
        
        return metrics
    
    @staticmethod
    def kick_detection_metrics(y_true: np.ndarray, y_pred: np.ndarray, 
                              y_proba: Optional[np.ndarray] = None) -> Dict[str, float]:
        """
        M√©triques sp√©cialis√©es pour la d√©tection de kicks
        
        Args:
            y_true: Vraies √©tiquettes (0: normal, 1: kick)
            y_pred: Pr√©dictions
            y_proba: Probabilit√©s de kick (optionnel)
            
        Returns:
            Dictionnaire avec m√©triques sp√©cialis√©es
        """
        metrics = {}
        
        # Matrice de confusion
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        
        # M√©triques de base
        metrics['true_positives'] = int(tp)
        metrics['true_negatives'] = int(tn)
        metrics['false_positives'] = int(fp)
        metrics['false_negatives'] = int(fn)
        
        # Taux de d√©tection (sensibilit√©)
        metrics['detection_rate'] = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        
        # Taux de fausses alarmes
        metrics['false_alarm_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        
        # Sp√©cificit√©
        metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        # Pr√©cision pour les kicks
        metrics['kick_precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        
        # Score F1 pour les kicks
        precision = metrics['kick_precision']
        recall = metrics['detection_rate']
        metrics['kick_f1'] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # Co√ªt pond√©r√© (les faux n√©gatifs sont plus co√ªteux)
        fn_cost_weight = 5  # Un kick manqu√© co√ªte 5 fois plus qu'une fausse alarme
        metrics['weighted_cost'] = fp + fn_cost_weight * fn
        
        return metrics

class DrillingSpecificMetrics:
    """
    M√©triques sp√©cifiques au domaine du forage
    """
    
    @staticmethod
    def rop_prediction_accuracy(y_true: np.ndarray, y_pred: np.ndarray, 
                               tolerance_percentage: float = 10.0) -> float:
        """
        Calcule la pr√©cision de pr√©diction du ROP avec tol√©rance
        
        Args:
            y_true: ROP r√©elles
            y_pred: ROP pr√©dites
            tolerance_percentage: Tol√©rance en pourcentage
            
        Returns:
            Pourcentage de pr√©dictions dans la tol√©rance
        """
        relative_errors = np.abs((y_true - y_pred) / np.maximum(y_true, 1e-8)) * 100
        within_tolerance = relative_errors <= tolerance_percentage
        
        return float(np.mean(within_tolerance)) * 100
    
    @staticmethod
    def formation_pressure_rmse(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        RMSE sp√©cialis√© pour la pression de formation
        
        Args:
            y_true: Pressions r√©elles
            y_pred: Pressions pr√©dites
            
        Returns:
            Dictionnaire avec RMSE et m√©triques d√©riv√©es
        """
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        
        # Statistiques des pressions
        pressure_range = np.max(y_true) - np.min(y_true)
        normalized_rmse = rmse / pressure_range if pressure_range > 0 else np.inf
        
        return {
            'rmse_psi': float(rmse),
            'normalized_rmse': float(normalized_rmse),
            'pressure_range_psi': float(pressure_range),
            'max_prediction_error_psi': float(np.max(np.abs(y_true - y_pred)))
        }
    
    @staticmethod
    def drilling_optimization_score(actual_params: Dict[str, np.ndarray], 
                                   predicted_params: Dict[str, np.ndarray],
                                   weights: Optional[Dict[str, float]] = None) -> float:
        """
        Score global d'optimisation du forage
        
        Args:
            actual_params: Param√®tres r√©els {param_name: values}
            predicted_params: Param√®tres pr√©dits {param_name: values}
            weights: Poids pour chaque param√®tre
            
        Returns:
            Score d'optimisation (0-100)
        """
        if weights is None:
            weights = {param: 1.0 for param in actual_params.keys()}
        
        param_scores = {}
        
        for param_name in actual_params.keys():
            if param_name in predicted_params:
                # Calculer le score normalis√© pour ce param√®tre
                mape = RegressionMetrics.mean_absolute_percentage_error(
                    actual_params[param_name], 
                    predicted_params[param_name]
                )
                
                # Convertir MAPE en score (100 - MAPE, plafonn√© √† 0)
                param_score = max(0, 100 - mape)
                param_scores[param_name] = param_score
        
        # Score pond√©r√© global
        total_weight = sum(weights.get(param, 1.0) for param in param_scores.keys())
        if total_weight == 0:
            return 0.0
        
        weighted_score = sum(
            param_scores[param] * weights.get(param, 1.0) 
            for param in param_scores.keys()
        ) / total_weight
        
        return float(weighted_score)

class ModelComparisonMetrics:
    """
    M√©triques pour comparer les mod√®les
    """
    
    @staticmethod
    def calculate_improvement_metrics(baseline_metrics: Dict[str, float],
                                    new_model_metrics: Dict[str, float]) -> Dict[str, float]:
        """
        Calcule les m√©triques d'am√©lioration par rapport √† un mod√®le de base
        
        Args:
            baseline_metrics: M√©triques du mod√®le de base
            new_model_metrics: M√©triques du nouveau mod√®le
            
        Returns:
            M√©triques d'am√©lioration
        """
        improvements = {}
        
        for metric in baseline_metrics.keys():
            if metric in new_model_metrics:
                baseline_val = baseline_metrics[metric]
                new_val = new_model_metrics[metric]
                
                if baseline_val != 0:
                    # Am√©lioration en pourcentage
                    if metric in ['mse', 'rmse', 'mae', 'mape']:  # M√©triques √† minimiser
                        improvement = ((baseline_val - new_val) / baseline_val) * 100
                    else:  # M√©triques √† maximiser (r2, accuracy, etc.)
                        improvement = ((new_val - baseline_val) / abs(baseline_val)) * 100
                    
                    improvements[f'{metric}_improvement_%'] = improvement
                
                improvements[f'{metric}_baseline'] = baseline_val
                improvements[f'{metric}_new'] = new_val
        
        return improvements
    
    @staticmethod
    def statistical_significance_test(y_true: np.ndarray, 
                                    pred1: np.ndarray, 
                                    pred2: np.ndarray) -> Dict[str, Any]:
        """
        Test de signification statistique entre deux mod√®les
        
        Args:
            y_true: Valeurs r√©elles
            pred1: Pr√©dictions mod√®le 1
            pred2: Pr√©dictions mod√®le 2
            
        Returns:
            R√©sultats du test statistique
        """
        from scipy.stats import wilcoxon, ttest_rel
        
        # Erreurs absolues
        errors1 = np.abs(y_true - pred1)
        errors2 = np.abs(y_true - pred2)
        
        # Test de Wilcoxon (non-param√©trique)
        try:
            wilcoxon_stat, wilcoxon_p = wilcoxon(errors1, errors2)
        except:
            wilcoxon_stat, wilcoxon_p = np.nan, np.nan
        
        # Test t appari√© (param√©trique)
        try:
            ttest_stat, ttest_p = ttest_rel(errors1, errors2)
        except:
            ttest_stat, ttest_p = np.nan, np.nan
        
        return {
            'wilcoxon_statistic': float(wilcoxon_stat) if not np.isnan(wilcoxon_stat) else None,
            'wilcoxon_p_value': float(wilcoxon_p) if not np.isnan(wilcoxon_p) else None,
            'ttest_statistic': float(ttest_stat) if not np.isnan(ttest_stat) else None,
            'ttest_p_value': float(ttest_p) if not np.isnan(ttest_p) else None,
            'model1_mean_error': float(np.mean(errors1)),
            'model2_mean_error': float(np.mean(errors2)),
            'significant_difference': float(wilcoxon_p) < 0.05 if not np.isnan(wilcoxon_p) else False
        }

class MetricsReporter:
    """
    Classe pour g√©n√©rer des rapports de m√©triques
    """
    
    @staticmethod
    def generate_regression_report(y_true: np.ndarray, y_pred: np.ndarray,
                                  model_name: str = "Model") -> str:
        """
        G√©n√®re un rapport de m√©triques pour la r√©gression
        
        Args:
            y_true: Valeurs r√©elles
            y_pred: Pr√©dictions
            model_name: Nom du mod√®le
            
        Returns:
            Rapport format√©
        """
        metrics = RegressionMetrics.calculate_all_metrics(y_true, y_pred)
        
        report = f"""
üìä RAPPORT DE M√âTRIQUES - {model_name}
{'='*50}

üéØ M√âTRIQUES PRINCIPALES:
   ‚Ä¢ R¬≤ Score:           {metrics['r2']:.4f}
   ‚Ä¢ RMSE:              {metrics['rmse']:.4f}
   ‚Ä¢ MAE:               {metrics['mae']:.4f}
   ‚Ä¢ MAPE:              {metrics['mape']:.2f}%

üìà M√âTRIQUES D√âTAILL√âES:
   ‚Ä¢ MSE:               {metrics['mse']:.4f}
   ‚Ä¢ SMAPE:             {metrics['smape']:.2f}%
   ‚Ä¢ Max Error:         {metrics['max_error']:.4f}
   ‚Ä¢ Median AE:         {metrics['median_ae']:.4f}

üîç ANALYSE DES R√âSIDUS:
   ‚Ä¢ Moyenne:           {metrics['residual_mean']:.4f}
   ‚Ä¢ √âcart-type:        {metrics['residual_std']:.4f}
   ‚Ä¢ Asym√©trie:         {metrics['residual_skew']:.4f}
        """
        
        # Ajouter m√©triques sp√©cifiques au forage si applicable
        if np.mean(y_true) > 0:  # Probablement du ROP
            efficiency = RegressionMetrics.drilling_efficiency_score(y_true, y_pred)
            accuracy_10 = DrillingSpecificMetrics.rop_prediction_accuracy(y_true, y_pred, 10.0)
            accuracy_20 = DrillingSpecificMetrics.rop_prediction_accuracy(y_true, y_pred, 20.0)
            
            report += f"""
‚ö° M√âTRIQUES FORAGE:
   ‚Ä¢ Efficacit√©:        {efficiency:.4f}
   ‚Ä¢ Pr√©cision ¬±10%:    {accuracy_10:.2f}%
   ‚Ä¢ Pr√©cision ¬±20%:    {accuracy_20:.2f}%
            """
        
        return report
    
    @staticmethod
    def generate_classification_report(y_true: np.ndarray, y_pred: np.ndarray,
                                     y_proba: Optional[np.ndarray] = None,
                                     model_name: str = "Model") -> str:
        """
        G√©n√®re un rapport de m√©triques pour la classification
        
        Args:
            y_true: Valeurs r√©elles
            y_pred: Pr√©dictions
            y_proba: Probabilit√©s (optionnel)
            model_name: Nom du mod√®le
            
        Returns:
            Rapport format√©
        """
        metrics = ClassificationMetrics.calculate_all_metrics(y_true, y_pred, y_proba)
        
        report = f"""
üìä RAPPORT DE CLASSIFICATION - {model_name}
{'='*50}

üéØ M√âTRIQUES PRINCIPALES:
   ‚Ä¢ Accuracy:          {metrics['accuracy']:.4f}
   ‚Ä¢ Precision (macro):  {metrics['precision_macro']:.4f}
   ‚Ä¢ Recall (macro):     {metrics['recall_macro']:.4f}
   ‚Ä¢ F1-Score (macro):   {metrics['f1_macro']:.4f}
        """
        
        if 'auc_roc' in metrics:
            report += f"   ‚Ä¢ AUC-ROC:           {metrics['auc_roc']:.4f}\n"
        
        if 'pr_auc' in metrics:
            report += f"   ‚Ä¢ AUC-PR:            {metrics['pr_auc']:.4f}\n"
        
        # Matrice de confusion
        cm = np.array(metrics['confusion_matrix'])
        if cm.shape == (2, 2):  # Classification binaire
            tn, fp, fn, tp = cm.ravel()
            report += f"""
üîç MATRICE DE CONFUSION:
   ‚Ä¢ True Negatives:    {tn}
   ‚Ä¢ False Positives:   {fp}
   ‚Ä¢ False Negatives:   {fn}
   ‚Ä¢ True Positives:    {tp}
            """
        
        return report

def create_metrics_dashboard_data(results: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
    """
    Cr√©e un DataFrame pour un dashboard de m√©triques
    
    Args:
        results: R√©sultats de plusieurs mod√®les {model_name: metrics_dict}
        
    Returns:
        DataFrame avec les m√©triques organis√©es
    """
    rows = []
    
    for model_name, metrics in results.items():
        row = {'Model': model_name}
        
        # Extraire les m√©triques principales
        for metric_name, value in metrics.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                row[metric_name] = value
        
        rows.append(row)
    
    return pd.DataFrame(rows)
    def calculate_all_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        Calcule toutes les m√©triques de r√©gression
        
        Args:
            y_true: Valeurs r√©elles
            y_pred: Pr√©dictions
            
        Returns:
            Dictionnaire avec toutes les m√©triques
        """
        metrics = {}
        
        # M√©triques de base
        metrics['mse'] = mean_squared_error(y_true, y_pred)
        metrics['rmse'] = np.sqrt(metrics['mse'])
        metrics['mae'] = mean_absolute_error(y_true, y_pred)
        metrics['r2'] = r2_score(y_true, y_pred)
        
        # M√©triques personnalis√©es
        metrics['mape'] = RegressionMetrics.mean_absolute_percentage_error(y_true, y_pred)
        metrics['smape'] = RegressionMetrics.symmetric_mean_absolute_percentage_error(y_true, y_pred)
        metrics['max_error'] = np.max(np.abs(y_true - y_pred))
        metrics['median_ae'] = np.median(np.abs(y_true - y_pred))
        
        # M√©triques de distribution des erreurs
        residuals = y_true - y_pred
        metrics['residual_mean'] = np.mean(residuals)
        metrics['residual_std'] = np.std(residuals)
        metrics['residual_skew'] = RegressionMetrics.calculate_skewness(residuals)
        
        return metrics
    
    @staticmethod
    def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Calcule le MAPE (Mean Absolute Percentage Error)
        
        Args:
            y_true: Valeurs r√©elles
            y_pred: Pr√©dictions
            
        Returns:
            MAPE en pourcentage
        """
        # √âviter la division par z√©ro
        mask = y_true != 0
        if not np.any(mask):
            return np.inf
        
        return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    
    @staticmethod
    def symmetric_mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Calcule le SMAPE (Symmetric Mean Absolute Percentage Error)
        
        Args:
            y_true: Valeurs r√©elles
            y_pred: Pr√©dictions
            
        Returns:
            SMAPE en pourcentage
        """
        numerator = np.abs(y_true - y_pred)
        denominator = (np.abs(y_true) + np.abs(y_pred)) / 2
        
        # √âviter la division par z√©ro
        mask = denominator != 0
        if not np.any(mask):
            return 0.0
        
        return np.mean(numerator[mask] / denominator[mask]) * 100